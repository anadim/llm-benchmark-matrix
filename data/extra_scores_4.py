"""
Extra scores batch 4: IFBench, LiveBench, SimpleBench, BigCodeBench,
Terminal-Bench 1.0, CritPt, MathArena competitions, IMO 2025.
"""

EXTRA_SCORES = [
    # ── IFBench (from AllenAI GitHub) ──
    ("o3-high", "ifbench", 69.3, "https://github.com/allenai/IFBench"),
    ("gemini-2.5-pro", "ifbench", 52.3, "https://github.com/allenai/IFBench"),
    ("claude-sonnet-4", "ifbench", 42.3, "https://github.com/allenai/IFBench"),
    ("deepseek-r1", "ifbench", 38.0, "https://github.com/allenai/IFBench"),
    ("qwen3-32b", "ifbench", 37.3, "https://github.com/allenai/IFBench"),
    ("qwen3-8b", "ifbench", 35.0, "https://github.com/allenai/IFBench"),

    # ── LiveBench — Jan 2026 overall scores ──
    ("claude-opus-4.5", "livebench", 76.0, "https://livebench.ai/"),
    ("gemini-3-pro", "livebench", 73.4, "https://livebench.ai/"),
    ("gemini-3-flash", "livebench", 72.4, "https://livebench.ai/"),
    ("gpt-5", "livebench", 70.5, "https://livebench.ai/"),
    ("deepseek-v3.2", "livebench", 62.2, "https://livebench.ai/"),
    ("grok-4.1", "livebench", 60.0, "https://livebench.ai/"),
    ("glm-4.7", "livebench", 58.1, "https://livebench.ai/"),
    ("gemini-2.5-pro", "livebench", 58.3, "https://livebench.ai/"),
    ("claude-opus-4.1", "livebench", 54.5, "https://livebench.ai/"),
    ("claude-sonnet-4.5", "livebench", 53.7, "https://livebench.ai/"),
    ("gemini-2.5-flash", "livebench", 47.7, "https://livebench.ai/"),
    ("claude-haiku-4.5", "livebench", 45.3, "https://livebench.ai/"),

    # ── SimpleBench (from LM Council, Feb 2026) ──
    ("gemini-3.1-pro", "simplebench", 79.6, "https://lmcouncil.ai/benchmarks"),
    ("claude-opus-4.6", "simplebench", 67.6, "https://lmcouncil.ai/benchmarks"),
    ("gemini-2.5-pro", "simplebench", 62.4, "https://lmcouncil.ai/benchmarks"),
    ("claude-opus-4.5", "simplebench", 62.0, "https://lmcouncil.ai/benchmarks"),
    ("gpt-5", "simplebench", 61.6, "https://lmcouncil.ai/benchmarks"),

    # ── BigCodeBench — Full Instruct pass@1 ──
    ("deepseek-v3", "bigcodebench", 50.0, "https://bigcode-bench.github.io/"),
    ("llama-4-maverick", "bigcodebench", 49.7, "https://bigcode-bench.github.io/"),
    ("gpt-4.1-mini", "bigcodebench", 48.9, "https://bigcode-bench.github.io/"),
    ("gemini-2.0-flash", "bigcodebench", 45.9, "https://bigcode-bench.github.io/"),
    ("phi-4", "bigcodebench", 45.5, "https://bigcode-bench.github.io/"),
    ("qwq-32b", "bigcodebench", 44.6, "https://bigcode-bench.github.io/"),
    ("command-a", "bigcodebench", 33.8, "https://bigcode-bench.github.io/"),

    # ── Terminal-Bench 1.0 — Terminus standardized agent scores ──
    ("claude-sonnet-4.5", "terminal_bench_1", 51.0, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("claude-opus-4.1", "terminal_bench_1", 43.8, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("minimax-m2", "terminal_bench_1", 42.0, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("gpt-5", "terminal_bench_1", 41.3, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("claude-opus-4", "terminal_bench_1", 39.0, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("grok-4", "terminal_bench_1", 39.0, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("claude-sonnet-4", "terminal_bench_1", 36.4, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("claude-3.7-sonnet", "terminal_bench_1", 35.2, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("gpt-4.1", "terminal_bench_1", 30.3, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("o3-high", "terminal_bench_1", 30.2, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("gemini-2.5-pro", "terminal_bench_1", 25.3, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("o4-mini-high", "terminal_bench_1", 18.5, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("grok-3-beta", "terminal_bench_1", 17.5, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("gemini-2.5-flash", "terminal_bench_1", 16.8, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("llama-4-maverick", "terminal_bench_1", 15.5, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("qwen3-32b", "terminal_bench_1", 15.5, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("qwen3-235b", "terminal_bench_1", 6.6, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),
    ("deepseek-r1", "terminal_bench_1", 5.7, "https://www.tbench.ai/leaderboard/terminal-bench/1.0"),

    # ── CritPt (from GitHub + Artificial Analysis) ──
    ("gemini-3.1-pro", "critpt", 17.7, "https://artificialanalysis.ai/evaluations/critpt"),
    ("claude-opus-4.6", "critpt", 12.6, "https://artificialanalysis.ai/evaluations/critpt"),
    ("gemini-3-pro", "critpt", 9.1, "https://github.com/CritPt-Benchmark/CritPt"),
    ("gpt-5", "critpt", 5.7, "https://github.com/CritPt-Benchmark/CritPt"),
    ("gemini-2.5-pro", "critpt", 2.0, "https://github.com/CritPt-Benchmark/CritPt"),
    ("o3-high", "critpt", 1.4, "https://github.com/CritPt-Benchmark/CritPt"),
    ("deepseek-r1", "critpt", 1.1, "https://github.com/CritPt-Benchmark/CritPt"),
    ("gemini-2.5-flash", "critpt", 1.1, "https://github.com/CritPt-Benchmark/CritPt"),
    ("o4-mini-high", "critpt", 0.6, "https://github.com/CritPt-Benchmark/CritPt"),
    ("claude-opus-4", "critpt", 0.3, "https://github.com/CritPt-Benchmark/CritPt"),
    ("llama-4-maverick", "critpt", 0.0, "https://github.com/CritPt-Benchmark/CritPt"),

    # ── MathArena competitions (cleaned model IDs) ──
    # HMMT Nov 2025
    ("gpt-5", "hmmt_nov_2025", 89.17, "https://matharena.ai/"),
    ("gpt-5.1", "hmmt_nov_2025", 91.67, "https://matharena.ai/"),
    ("gpt-5.2", "hmmt_nov_2025", 95.83, "https://matharena.ai/"),
    ("gemini-3-pro", "hmmt_nov_2025", 93.33, "https://matharena.ai/"),
    ("gemini-2.5-pro", "hmmt_nov_2025", 66.67, "https://matharena.ai/"),
    ("deepseek-v3.2-speciale", "hmmt_nov_2025", 93.33, "https://matharena.ai/"),
    ("deepseek-v3.2", "hmmt_nov_2025", 90.0, "https://matharena.ai/"),
    ("grok-4", "hmmt_nov_2025", 88.33, "https://matharena.ai/"),
    ("grok-4.1", "hmmt_nov_2025", 93.33, "https://matharena.ai/"),
    ("kimi-k2-thinking", "hmmt_nov_2025", 89.17, "https://matharena.ai/"),
    ("kimi-k2.5", "hmmt_nov_2025", 89.17, "https://matharena.ai/"),
    ("gemini-3-flash", "hmmt_nov_2025", 93.33, "https://matharena.ai/"),

    # BRUMO 2025
    ("gpt-5", "brumo_2025", 91.67, "https://matharena.ai/"),
    ("gpt-5.1", "brumo_2025", 93.33, "https://matharena.ai/"),
    ("gpt-5.2", "brumo_2025", 98.33, "https://matharena.ai/"),
    ("gemini-3-flash", "brumo_2025", 100.0, "https://matharena.ai/"),
    ("gemini-3-pro", "brumo_2025", 98.33, "https://matharena.ai/"),
    ("gemini-2.5-pro", "brumo_2025", 90.0, "https://matharena.ai/"),
    ("gemini-2.5-flash", "brumo_2025", 83.33, "https://matharena.ai/"),
    ("deepseek-v3.2-speciale", "brumo_2025", 99.17, "https://matharena.ai/"),
    ("deepseek-v3.2", "brumo_2025", 96.67, "https://matharena.ai/"),
    ("deepseek-r1", "brumo_2025", 80.83, "https://matharena.ai/"),
    ("deepseek-r1-0528", "brumo_2025", 92.5, "https://matharena.ai/"),
    ("grok-4", "brumo_2025", 95.0, "https://matharena.ai/"),
    ("grok-4.1", "brumo_2025", 97.5, "https://matharena.ai/"),
    ("o3-high", "brumo_2025", 95.83, "https://matharena.ai/"),
    ("o4-mini-high", "brumo_2025", 86.67, "https://matharena.ai/"),
    ("claude-3.7-sonnet", "brumo_2025", 65.83, "https://matharena.ai/"),
    ("claude-sonnet-4.5", "brumo_2025", 90.83, "https://matharena.ai/"),
    ("kimi-k2-thinking", "brumo_2025", 93.33, "https://matharena.ai/"),
    ("kimi-k2.5", "brumo_2025", 98.33, "https://matharena.ai/"),

    # SMT 2025
    ("gpt-5", "smt_2025", 91.98, "https://matharena.ai/"),
    ("gpt-5.1", "smt_2025", 91.04, "https://matharena.ai/"),
    ("gpt-5.2", "smt_2025", 91.98, "https://matharena.ai/"),
    ("gemini-3-flash", "smt_2025", 92.92, "https://matharena.ai/"),
    ("gemini-3-pro", "smt_2025", 93.4, "https://matharena.ai/"),
    ("gemini-2.5-pro", "smt_2025", 84.91, "https://matharena.ai/"),
    ("gemini-2.5-flash", "smt_2025", 75.47, "https://matharena.ai/"),
    ("deepseek-v3.2-speciale", "smt_2025", 89.15, "https://matharena.ai/"),
    ("deepseek-v3.2", "smt_2025", 87.74, "https://matharena.ai/"),
    ("deepseek-r1", "smt_2025", 66.51, "https://matharena.ai/"),
    ("deepseek-r1-0528", "smt_2025", 83.02, "https://matharena.ai/"),
    ("grok-4", "smt_2025", 85.85, "https://matharena.ai/"),
    ("grok-4.1", "smt_2025", 84.6, "https://matharena.ai/"),
    ("o3-high", "smt_2025", 87.74, "https://matharena.ai/"),
    ("o4-mini-high", "smt_2025", 88.68, "https://matharena.ai/"),
    ("claude-3.7-sonnet", "smt_2025", 56.6, "https://matharena.ai/"),
    ("claude-sonnet-4.5", "smt_2025", 83.96, "https://matharena.ai/"),
    ("kimi-k2-thinking", "smt_2025", 91.04, "https://matharena.ai/"),
    ("kimi-k2.5", "smt_2025", 90.57, "https://matharena.ai/"),

    # CMIMC 2025
    ("gpt-5", "cmimc_2025", 90.0, "https://matharena.ai/"),
    ("gpt-5.1", "cmimc_2025", 91.88, "https://matharena.ai/"),
    ("gpt-5.2", "cmimc_2025", 91.25, "https://matharena.ai/"),
    ("gemini-3-flash", "cmimc_2025", 90.62, "https://matharena.ai/"),
    ("gemini-3-pro", "cmimc_2025", 90.0, "https://matharena.ai/"),
    ("gemini-2.5-pro", "cmimc_2025", 58.13, "https://matharena.ai/"),
    ("gemini-2.5-flash", "cmimc_2025", 50.62, "https://matharena.ai/"),
    ("deepseek-v3.2-speciale", "cmimc_2025", 94.38, "https://matharena.ai/"),
    ("deepseek-v3.2", "cmimc_2025", 83.75, "https://matharena.ai/"),
    ("deepseek-r1-0528", "cmimc_2025", 69.38, "https://matharena.ai/"),
    ("grok-4", "cmimc_2025", 83.75, "https://matharena.ai/"),
    ("grok-4.1", "cmimc_2025", 84.38, "https://matharena.ai/"),
    ("o3-high", "cmimc_2025", 79.38, "https://matharena.ai/"),
    ("o4-mini-high", "cmimc_2025", 84.38, "https://matharena.ai/"),
    ("claude-sonnet-4.5", "cmimc_2025", 66.88, "https://matharena.ai/"),
    ("kimi-k2-thinking", "cmimc_2025", 91.88, "https://matharena.ai/"),
    ("kimi-k2.5", "cmimc_2025", 91.25, "https://matharena.ai/"),

    # IMO 2025 (new models)
    ("deepseek-v3.2-speciale", "imo_2025", 83.3, "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"),
    ("deepseek-r1-0528", "imo_2025", 6.85, "https://matharena.ai/imo/"),
    ("gpt-5", "imo_2025", 38.1, "https://matharena.ai/"),
    ("o4-mini-high", "imo_2025", 14.29, "https://matharena.ai/"),

    # USAMO 2025 (additional)
    ("deepseek-r1-0528", "usamo_2025", 30.06, "https://matharena.ai/"),
    ("deepseek-r1", "usamo_2025", 4.76, "https://matharena.ai/"),
    ("claude-3.7-sonnet", "usamo_2025", 3.65, "https://matharena.ai/"),
    ("o3-mini-high", "usamo_2025", 2.08, "https://matharena.ai/"),

    # MathArena Apex 2025 (additional)
    ("gemini-3.1-pro", "matharena_apex_2025", 60.94, "https://matharena.ai/"),
    ("gemini-3-pro", "matharena_apex_2025", 23.44, "https://matharena.ai/"),
    ("gemini-3-flash", "matharena_apex_2025", 15.62, "https://matharena.ai/"),
    ("gpt-5.2", "matharena_apex_2025", 13.54, "https://matharena.ai/"),
    ("deepseek-v3.2-speciale", "matharena_apex_2025", 9.38, "https://matharena.ai/"),
    ("kimi-k2.5", "matharena_apex_2025", 8.85, "https://matharena.ai/"),
    ("grok-4.1", "matharena_apex_2025", 5.21, "https://matharena.ai/"),
    ("qwen3-235b", "matharena_apex_2025", 5.21, "https://matharena.ai/"),
    ("deepseek-v3.2", "matharena_apex_2025", 2.08, "https://matharena.ai/"),
    ("grok-4", "matharena_apex_2025", 2.08, "https://matharena.ai/"),
    ("claude-sonnet-4.5", "matharena_apex_2025", 1.56, "https://matharena.ai/"),
    ("gpt-5", "matharena_apex_2025", 1.04, "https://matharena.ai/"),
    ("gpt-5.1", "matharena_apex_2025", 1.04, "https://matharena.ai/"),
    ("deepseek-r1-0528", "matharena_apex_2025", 1.04, "https://matharena.ai/"),
    ("gemini-2.5-pro", "matharena_apex_2025", 0.52, "https://matharena.ai/"),
    ("kimi-k2-thinking", "matharena_apex_2025", 0.0, "https://matharena.ai/"),

    # ── MRCR v2 (128K context, comparable) ──
    ("claude-opus-4.6", "mrcr_v2", 84.9, "https://www.nxcode.io/en/resources/news/gemini-3-1-pro-vs-claude-opus-4-6-vs-gpt-5-comparison-2026"),
    ("gemini-3.1-pro", "mrcr_v2", 84.9, "https://www.nxcode.io/en/resources/news/gemini-3-1-pro-vs-claude-opus-4-6-vs-gpt-5-comparison-2026"),
    ("gemini-2.5-pro", "mrcr_v2", 83.1, "https://www.vellum.ai/blog/google-gemini-3-benchmarks"),
    ("claude-sonnet-4.6", "mrcr_v2", 82.0, "https://awesomeagents.ai/leaderboards/long-context-benchmarks-leaderboard/"),
    ("gpt-4.1", "mrcr_v2", 80.0, "https://awesomeagents.ai/leaderboards/long-context-benchmarks-leaderboard/"),

    # ── AA Long Context Reasoning (additional) ──
    ("gpt-5", "aa_lcr", 75.6, "https://artificialanalysis.ai/evaluations/artificial-analysis-long-context-reasoning"),
    ("o3-high", "aa_lcr", 69.0, "https://artificialanalysis.ai/evaluations/artificial-analysis-long-context-reasoning"),
    ("grok-4", "aa_lcr", 68.0, "https://artificialanalysis.ai/evaluations/artificial-analysis-long-context-reasoning"),
    ("qwen3-235b", "aa_lcr", 67.0, "https://artificialanalysis.ai/evaluations/artificial-analysis-long-context-reasoning"),
]
